{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74beed8-0e15-4c90-a987-7073b6e410b5",
   "metadata": {},
   "source": [
    "# Detecting Hallucinations in Question-Answering System\n",
    "\n",
    "In this notebook I use the 'DeepPavlov/rubert-base-cased' BERT model for sequence classification, pre-trained on a Russian language corpus, and fine-tune it on my dataset for the binary classification task of hallucination detection.\n",
    "\n",
    "Hallucinations, in this context, refer to incorrect or misleading information produced by models when generating responses to given queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ec5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692581bc-a739-46aa-9312-bbd135bb8b1c",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The dataset used is a Russian-language question-answering system, structured to support the task of detecting hallucinations in text. It consists of the following columns:\n",
    "\n",
    "- summary: This column contains a summary or context related to the question and answer. It provides background information that may help in understanding whether the answer is relevant and correct.\n",
    "- question: This column contains the question that is posed based on the summary.\n",
    "- answer: This column contains the answer provided for the corresponding question.\n",
    "- is_hallucination: This column is a binary label indicating whether the answer is a -hallucination (1) or not (0). A hallucination in this context refers to an incorrect or misleading answer that does not correctly respond to the question based on the provided summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383319c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>is_hallucination</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Херманус Питер (Дик) Логгере (нидерл. Hermanus...</td>\n",
       "      <td>В каком городе проходил чемпионат мира по хокк...</td>\n",
       "      <td>В Хилверсюме.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ходуткинские горячие источники (Худутские горя...</td>\n",
       "      <td>Как называется район в который входят источники?</td>\n",
       "      <td>Елизовским районом</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Чёрная вдова (лат. Latrodectus mactans) — вид ...</td>\n",
       "      <td>Для кого опасны пауки-бокоходы?</td>\n",
       "      <td>Для рыб.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Рысь — река в России, протекает по территориям...</td>\n",
       "      <td>Какова длина реки Рысь?</td>\n",
       "      <td>5 км.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>И́се (яп. 伊勢市), ранее Удзиямада — город в Япон...</td>\n",
       "      <td>Что такое Исе?</td>\n",
       "      <td>Исе — это небольшой город в Японии, который не...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   summary  \\\n",
       "line_id                                                      \n",
       "0        Херманус Питер (Дик) Логгере (нидерл. Hermanus...   \n",
       "1        Ходуткинские горячие источники (Худутские горя...   \n",
       "2        Чёрная вдова (лат. Latrodectus mactans) — вид ...   \n",
       "3        Рысь — река в России, протекает по территориям...   \n",
       "4        И́се (яп. 伊勢市), ранее Удзиямада — город в Япон...   \n",
       "\n",
       "                                                  question  \\\n",
       "line_id                                                      \n",
       "0        В каком городе проходил чемпионат мира по хокк...   \n",
       "1         Как называется район в который входят источники?   \n",
       "2                          Для кого опасны пауки-бокоходы?   \n",
       "3                                  Какова длина реки Рысь?   \n",
       "4                                           Что такое Исе?   \n",
       "\n",
       "                                                    answer  is_hallucination  \n",
       "line_id                                                                       \n",
       "0                                            В Хилверсюме.                 1  \n",
       "1                                       Елизовским районом                 0  \n",
       "2                                                 Для рыб.                 1  \n",
       "3                                                    5 км.                 1  \n",
       "4        Исе — это небольшой город в Японии, который не...                 1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv', index_col = 0 )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad2580-e4e7-40b8-982f-63dd123d2102",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "Let's print several samples of hallucinated and non-hallucinated data to get a sense of the text content and format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea35237d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:13:58.457713Z",
     "iopub.status.busy": "2024-07-23T14:13:58.457468Z",
     "iopub.status.idle": "2024-07-23T14:13:58.467306Z",
     "shell.execute_reply": "2024-07-23T14:13:58.466403Z",
     "shell.execute_reply.started": "2024-07-23T14:13:58.457691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q В каком городе проходил чемпионат мира по хоккею с шайбой в 1936 году?\n",
      "A В Хилверсюме.\n",
      "S Херманус Питер (Дик) Логгере (нидерл. Hermanus Pieter (Dick) Loggere, 6 мая 1921, Амстердам, Нидерланды — 30 декабря 2014, Хилверсюм, Нидерланды) — нидерландский хоккеист (хоккей на траве), полузащитник. Бронзовый призёр летних Олимпийских игр 1948 года.\n",
      "\n",
      "Q Для кого опасны пауки-бокоходы?\n",
      "A Для рыб.\n",
      "S Чёрная вдова (лат. Latrodectus mactans) — вид пауков, распространённый в Северной и Южной Америке. Опасен для человека.\n",
      "\n",
      "Q Какова длина реки Рысь?\n",
      "A 5 км.\n",
      "S Рысь — река в России, протекает по территориям Муезерского городского поселения и Ледмозерского сельского поселения Муезерского района Карелии. Устье реки находится в 6,6 км по правому берегу реки Кайдодеги. Длина реки — 10 км. Рысь течёт преимущественно в северном направлении по заболоченной территории. Впадает в реку Кайдодеги возле озера Кайдодеги. Населённые пункты на реке отсутствуют.\n",
      "\n",
      "Q Что такое Исе?\n",
      "A Исе — это небольшой город в Японии, который не входит в состав Национального парка Исе-Сима. Население — 98,819 человек (2003); город занимает площадь 178.97 км².\n",
      "S И́се (яп. 伊勢市), ранее Удзиямада — город в Японии в префектуре Миэ. Исе входит в состав Национального парка Исе-Сима. Население — 98,819 человек (2003); город занимает площадь 178.97 км². Крупный центр туризма и паломничества.  Статус большого города Удзиямада получил 1 сентября 1906 года. После объединения с несколькими соседними городами 1 января 1955 года был образован город Исе.\n",
      "\n",
      "Q Сколько человек погибло на борту?\n",
      "A 98 человек.\n",
      "S Катастрофа A310 в Иркутске — крупная авиационная катастрофа, произошедшая в воскресенье 9 июля 2006 года. Авиалайнер Airbus А310-324 авиакомпании «Сибирь» совершал внутренний рейс SBI778 по маршруту Москва—Иркутск, но после посадки в пункте назначения выкатился за пределы взлётной полосы аэропорта Иркутска и врезался в гаражный комплекс. Из находившихся на его борту 203 человек (195 пассажиров и 8 членов экипажа) погибли 125. Расследование Межгосударственного авиационного комитета (МАК) показало, что самолёт был сброшен с ВПП двигателем №1 (левый), внезапно перешедшим во взлётный режим в момент включения реверса двигателя №2 (правый).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    line = data[data['is_hallucination']==1].iloc[i,:]\n",
    "    print(f\"Q {line['question']}\")\n",
    "    print(f\"A {line['answer']}\")\n",
    "    print(f\"S {line['summary']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4be6caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:13:58.469686Z",
     "iopub.status.busy": "2024-07-23T14:13:58.469412Z",
     "iopub.status.idle": "2024-07-23T14:13:58.478855Z",
     "shell.execute_reply": "2024-07-23T14:13:58.478012Z",
     "shell.execute_reply.started": "2024-07-23T14:13:58.469662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Как называется район в который входят источники?\n",
      "A Елизовским районом\n",
      "S Ходуткинские горячие источники (Худутские горячие источники, Термальные источники вулкана Ходутка, Ходуткинское геотермальное месторождение) — пресные геотермальные источники на юге полуострова Камчатка в Елизовском районе Камчатского края. Относятся к Южно-Камчатской геотермальной провинции.\n",
      "\n",
      "Q Как точно переводится название кабаре Мулен Руж?\n",
      "A Красная мельница\n",
      "S «Муле́н Ру́ж» (фр. Moulin Rouge, буквально «Красная мельница») — классическое кабаре в Париже, построенное в 1889 году, одна из достопримечательностей французской столицы. Расположено в 18 муниципальном округе, на бульваре Клиши, в квартале красных фонарей около площади Пигаль. Ближайшая станция метро — линия 2, станция «Бланш».\n",
      "\n",
      "Q Как называлась кинолента, вышедшая на экраны в 1961 году?\n",
      "A Дуэль\n",
      "S «Дуэль» (в советском прокате шёл под названием «Комиссар полиции и Малыш», рум. Duelul) — первый по хронологии и предпоследний по времени съёмок фильм из цикла приключений комиссара Миклована. Играющему главную роль 29-летнего комиссара полиции, румынскому режиссёру Серджиу Николаеску уже было за пятьдесят.\n",
      "\n",
      "Q В каком году появилось первое появление персонажа комиксов?\n",
      "A в феврале 1976 года\n",
      "S Пауэр Гёрл (англ. Power Girl, настоящее имя Кара Зор-Л, также известная как Карен Старр) — персонаж комиксов издательства DC Comics, первое появление которой состоялось в «All Star Comics» № 58 (январь/февраль, 1976 год). Пауэр Гёрл — это аналог Супергёрл c Земли-2, находящейся в параллельной вселенной в Мультивселенной DC Comics. Её двоюродным братом также является Супермен — Кал-Л, двойник Кларка Кента с докризисной Земли-2. Перед гибелью Криптона, её родной планеты, родители Кары отправили её на космическом корабле в направлении Земли, что позволило ей спастись, как и её кузену. И хотя она покинула планету одновременно с Суперменом, её кораблю потребовалось больше времени, чтобы достигнуть Земли-2, из-за этого она появляется на Земле гораздо позже него. Как и все криптонцы, она обладает сверхчеловеческой силой, не сравнимой с человеческой, и умеет летать, является членом Лиги справедливости Америки и первым её председателем. У неё спортивное телосложение, светлые волосы, классическая прическа — каре, одевается в отличительный бело-красно-синий костюм, у неё агрессивный стиль ведения боя. На протяжении всех выпусков All Star Comics, где состоялось её первое появление, она часто конфликтует с Диким Котом (персонажем комиксов DC), который относится к ней поначалу довольно пренебрежительно. В ограниченных выпусках серии «Кризис на Бесконечных Землях» (англ. Crisis on Infinite Earths) 1985 года по сюжету была уничтожена Земля-2, в результате чего её биографию, как и биографии многих выживших после этих событий, изменили, криптонское происхождение было скрыто и она стала внучкой колдуна Ариона, жившего в Атлантиде. Однако в ходе событий, описанных в выпусках ограниченной серии «Бесконечный Кризис» (англ. Infinite Crisis) 2005—2006 годов, восстановлен её статус беженца с Криптона, разрушенного во вселенной докризисной Земли-2.\n",
      "\n",
      "Q В каком году была удостоена звания доктор наук?\n",
      "A в 1991 году\n",
      "S Мамедова (Ибpагимбейли) Сона Ибpагим кызы (1918-1991) — врач и специалист в области особо опасных инфекций, Заслуженный врач Азербайджана.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    line = data[data['is_hallucination']==0].iloc[i,:]\n",
    "    print(f\"Q {line['question']}\")\n",
    "    print(f\"A {line['answer']}\")\n",
    "    print(f\"S {line['summary']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935bff3-ec9e-46ae-ae09-5dfd79c9728f",
   "metadata": {},
   "source": [
    "## Tokenization:\n",
    "\n",
    "For tokenization, I use BertTokenizer, compatible with the BERT model, pre-trained on  Russian language corpus. In general, tokenization process is as follows:\n",
    "- Summary, question, and answer fields are concatenated into one string so that the model can get the context of all the information. Special tokens [CLS] and [SEP] are added:\n",
    "    - The [CLS] token is added at the beginning of the entire sequence.\n",
    "    - The [SEP] token is used to separate different segments of text, such as summary, question, and answer.\n",
    "- token_type_ids is used to differentiate between different parts of the text (summary, question, and answer). \n",
    "  \n",
    "I do not lowercase text, as BERT is case-sensitive and was trained on case-sensitive text. Also, I do not remove special characters and punctuation, as they help the model to better understand context.\n",
    "\n",
    "Maximum token length for the BERT model is 512 tokens. There are several methods to handle large texts:\n",
    "- Text summarization to extract key points from a long text. \n",
    "- Splitting text into several parts within length limit, processing each part separately, and combining the results.\n",
    "- Processing long text in overlapping windows to account for contextual transitions between parts of the text.\n",
    "   \n",
    "I chose simple cropping to the maximum token length, as most examples in the dataset are within 512 tokens. Each segment (summary, question, and answer) of the text is cropped proportionally, so that the relative proportions of each part are maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7d97254",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:13:58.480397Z",
     "iopub.status.busy": "2024-07-23T14:13:58.480110Z",
     "iopub.status.idle": "2024-07-23T14:13:58.825063Z",
     "shell.execute_reply": "2024-07-23T14:13:58.823971Z",
     "shell.execute_reply.started": "2024-07-23T14:13:58.480375Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "max_length = 512  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "509fd5bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:13:58.827048Z",
     "iopub.status.busy": "2024-07-23T14:13:58.826681Z",
     "iopub.status.idle": "2024-07-23T14:14:02.355578Z",
     "shell.execute_reply": "2024-07-23T14:14:02.354756Z",
     "shell.execute_reply.started": "2024-07-23T14:13:58.827021Z"
    }
   },
   "outputs": [],
   "source": [
    "data['text'] = data.apply(lambda row: f\"[CLS] {row['summary']} [SEP] {row['question']} [SEP] {row['answer']} [SEP]\", axis=1)\n",
    "token_len = [len(tokenizer.tokenize(t)) for t in data['text']]\n",
    "token_len.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f31638ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:14:02.357275Z",
     "iopub.status.busy": "2024-07-23T14:14:02.356925Z",
     "iopub.status.idle": "2024-07-23T14:14:02.363937Z",
     "shell.execute_reply": "2024-07-23T14:14:02.363079Z",
     "shell.execute_reply.started": "2024-07-23T14:14:02.357242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[823, 436, 435, 425, 419, 417, 416, 412, 409, 408]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_len[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f527d-ddec-42d2-88e7-70c6869fa1e9",
   "metadata": {},
   "source": [
    "The dataset is randomly split into training and testing sets, with the test set size being 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1edf61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:14:02.365435Z",
     "iopub.status.busy": "2024-07-23T14:14:02.365079Z",
     "iopub.status.idle": "2024-07-23T14:14:02.377694Z",
     "shell.execute_reply": "2024-07-23T14:14:02.376657Z",
     "shell.execute_reply.started": "2024-07-23T14:14:02.365402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 210, 840, 210)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(data['text'], data['is_hallucination'], test_size=0.2, random_state=42)\n",
    "\n",
    "len(train_texts), len(test_texts), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f160d39c-e839-493a-bfe6-3073d31d0f24",
   "metadata": {},
   "source": [
    "Custom function `tokenize_segments` tokenizes and encodes a single input text into the format required for BERT. It consists of the following steps:\n",
    "- Input text is splitted back into: summary, question, and answer parts.\n",
    "- Each part is tokenized separately using the BERT tokenizer.\n",
    "- Handling length constraints: If the combined length of tokens exceeds the max_length limit (considering special tokens), the tokens for each segment (summary, question, and answer) are truncated proportionally to fit within the limit.\n",
    "- Combining tokens back, adding special tokens.\n",
    "- Converting tokens to IDs.\n",
    "- Token type IDs are created to distinguish between different segments. Since the tokenizer from the transformers library does not support using more than two segments, summary and question are combined into one segment. Here, 0 indicates the first segment (summary and question), and 1 indicates the second segment (answer). \n",
    "- The attention mask is created to indicate which tokens should be attended to (1 for real tokens, 0 for padding).\n",
    "- Sequences are padded to the maximum length if they are shorter.\n",
    "  \n",
    "The function returns a dictionary containing the input_ids, attention_mask, and token_type_ids as tensors.\n",
    "  \n",
    "Function `encode_data` processes multiple texts using `tokenize_segments` and stacks the results into batches suitable for model input. The function returns the dictionary of batched tensors, ready for input into a BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e79e92b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:14:02.379290Z",
     "iopub.status.busy": "2024-07-23T14:14:02.378986Z",
     "iopub.status.idle": "2024-07-23T14:14:02.398080Z",
     "shell.execute_reply": "2024-07-23T14:14:02.397166Z",
     "shell.execute_reply.started": "2024-07-23T14:14:02.379250Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_segments(text, tokenizer, max_length=512):\n",
    "    \n",
    "    text_parts = text.replace('[CLS]','').split('[SEP]')\n",
    "\n",
    "    summary, question, answer = text_parts[0].strip(), text_parts[1].strip(), text_parts[2].strip()\n",
    "    \n",
    "    summary_tokens = tokenizer.tokenize(summary)\n",
    "    question_tokens = tokenizer.tokenize(question)\n",
    "    answer_tokens = tokenizer.tokenize(answer)\n",
    "    \n",
    "    if len(summary_tokens) + len(question_tokens) + len(answer_tokens) > max_length - 4:\n",
    "        extra = len(summary_tokens) + len(question_tokens) + len(answer_tokens) - (max_length - 4)\n",
    "        n = int(( max_length - 4)*len(summary_tokens)/(len(summary_tokens) + len(question_tokens) + len(answer_tokens)))\n",
    "        m = int((max_length - 4)*len(question_tokens)/(len(summary_tokens) + len(question_tokens) + len(answer_tokens)))\n",
    "        k =  (max_length - 4) - n - m\n",
    "        summary_tokens = summary_tokens[:n]\n",
    "        question_tokens = question_tokens[:m]\n",
    "        answer_tokens = answer_tokens[:k]\n",
    "    \n",
    "    tokens = ['[CLS]'] + summary_tokens + ['[SEP]'] + question_tokens + ['[SEP]'] + answer_tokens + ['[SEP]']\n",
    "    \n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    token_type_ids = [0] * (len(summary_tokens) + 2) + [0] * (len(question_tokens) + 1) + [1] * (len(answer_tokens) + 1)\n",
    "    \n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([0] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        token_type_ids = token_type_ids[:max_length]\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids),\n",
    "        'attention_mask': torch.tensor(attention_mask),\n",
    "        'token_type_ids': torch.tensor(token_type_ids)\n",
    "    }\n",
    "\n",
    "def encode_data(texts, tokenizer):\n",
    "    encodings = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'token_type_ids': []\n",
    "    }\n",
    "\n",
    "    for text in texts:\n",
    "        text_encoding = tokenize_segments(text, tokenizer)\n",
    "        for key in encodings:\n",
    "            encodings[key].append(text_encoding[key])\n",
    "\n",
    "    for key in encodings:\n",
    "        encodings[key] = torch.stack(encodings[key])\n",
    "        \n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c965a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:14:02.401603Z",
     "iopub.status.busy": "2024-07-23T14:14:02.401265Z",
     "iopub.status.idle": "2024-07-23T14:14:06.555830Z",
     "shell.execute_reply": "2024-07-23T14:14:06.554834Z",
     "shell.execute_reply.started": "2024-07-23T14:14:02.401567Z"
    }
   },
   "outputs": [],
   "source": [
    "train_encodings = encode_data(train_texts, tokenizer)\n",
    "test_encodings = encode_data(test_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44d4fcba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:14:06.557432Z",
     "iopub.status.busy": "2024-07-23T14:14:06.557074Z",
     "iopub.status.idle": "2024-07-23T14:14:06.564058Z",
     "shell.execute_reply": "2024-07-23T14:14:06.563111Z",
     "shell.execute_reply.started": "2024-07-23T14:14:06.557401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([840, 512]), torch.Size([840, 512]), torch.Size([840, 512]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['input_ids'].shape, train_encodings['attention_mask'].shape, train_encodings['token_type_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e870dfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:14:06.565580Z",
     "iopub.status.busy": "2024-07-23T14:14:06.565278Z",
     "iopub.status.idle": "2024-07-23T14:14:06.573838Z",
     "shell.execute_reply": "2024-07-23T14:14:06.573037Z",
     "shell.execute_reply.started": "2024-07-23T14:14:06.565556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([210, 512]), torch.Size([210, 512]), torch.Size([210, 512]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encodings['input_ids'].shape, test_encodings['attention_mask'].shape, test_encodings['token_type_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7dfa8ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:14:06.575330Z",
     "iopub.status.busy": "2024-07-23T14:14:06.574970Z",
     "iopub.status.idle": "2024-07-23T14:14:06.583452Z",
     "shell.execute_reply": "2024-07-23T14:14:06.582693Z",
     "shell.execute_reply.started": "2024-07-23T14:14:06.575299Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = train_labels.reset_index(drop=True)\n",
    "test_labels = test_labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc730fa4-7c10-4119-84d8-c99a036cf5dd",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "\n",
    "A custom Dataset class and DataLoader are defined for the training and testing data, facilitating batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed790173",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:14:06.585116Z",
     "iopub.status.busy": "2024-07-23T14:14:06.584636Z",
     "iopub.status.idle": "2024-07-23T14:14:06.593937Z",
     "shell.execute_reply": "2024-07-23T14:14:06.593168Z",
     "shell.execute_reply.started": "2024-07-23T14:14:06.585084Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b523fe-5545-43d5-84a2-8a4f861a173a",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "I load pre-trained on Russian language corpus BERT model for sequence classification and fine-tune it on my dataset for a binary classification task of hallucination detection. \n",
    "\n",
    "I use AdamW optimizer with weight decay to add regularization and scheduler to adjust the learning rate during training, with a warm-up period and a linear decay. \n",
    "\n",
    "I train the model for 3 epochs to avoid overfitting. BERT models typically use cross-entropy loss for optimization in classification tasks. \n",
    "\n",
    "During each epoch, the training loss, accuracy, and F1-score are computed and printed. The model is also evaluated on the test set after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d4de50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:14:06.595657Z",
     "iopub.status.busy": "2024-07-23T14:14:06.595139Z",
     "iopub.status.idle": "2024-07-23T14:14:06.609782Z",
     "shell.execute_reply": "2024-07-23T14:14:06.608892Z",
     "shell.execute_reply.started": "2024-07-23T14:14:06.595626Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for batch in tqdm(data_loader, desc='Train'):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    total_loss /= len(data_loader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    \n",
    "    return total_loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "\n",
    "    for batch in tqdm(data_loader, desc='Evaluation'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "    total_loss /= len(data_loader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    \n",
    "    return total_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02a77780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:14:06.611243Z",
     "iopub.status.busy": "2024-07-23T14:14:06.610922Z",
     "iopub.status.idle": "2024-07-23T14:14:08.491996Z",
     "shell.execute_reply": "2024-07-23T14:14:08.491068Z",
     "shell.execute_reply.started": "2024-07-23T14:14:06.611219Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.02)\n",
    "total_steps = len(train_loader) * num_epochs  \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dba6b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:14:08.493863Z",
     "iopub.status.busy": "2024-07-23T14:14:08.493258Z",
     "iopub.status.idle": "2024-07-23T14:18:38.967933Z",
     "shell.execute_reply": "2024-07-23T14:18:38.966795Z",
     "shell.execute_reply.started": "2024-07-23T14:14:08.493802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 105/105 [01:22<00:00,  1.27it/s]\n",
      "Evaluation: 100%|██████████| 27/27 [00:06<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.73 | Valid loss: 0.65 \n",
      "Train accuracy: 0.49 | Valid accuracy: 0.61\n",
      "Train F1: 0.28 | Valid F1: 0.38\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 105/105 [01:22<00:00,  1.27it/s]\n",
      "Evaluation: 100%|██████████| 27/27 [00:06<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.45 | Valid loss: 0.38 \n",
      "Train accuracy: 0.85 | Valid accuracy: 0.87\n",
      "Train F1: 0.84 | Valid F1: 0.86\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 105/105 [01:22<00:00,  1.27it/s]\n",
      "Evaluation: 100%|██████████| 27/27 [00:06<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.20 | Valid loss: 0.37 \n",
      "Train accuracy: 0.94 | Valid accuracy: 0.91\n",
      "Train F1: 0.94 | Valid F1: 0.91\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    train_loss, train_accuracy, __, __, train_f1 = train_model(model, train_loader, optimizer, device, scheduler)\n",
    "    valid_loss, valid_accuracy, __, __, valid_f1 = eval_model(model, test_loader, device)\n",
    "    print(f'Train loss: {train_loss:.2f} | Valid loss: {valid_loss:.2f} ')\n",
    "    print(f'Train accuracy: {train_accuracy:.2f} | Valid accuracy: {valid_accuracy:.2f}')\n",
    "    print(f'Train F1: {train_f1:.2f} | Valid F1: {valid_f1:.2f}')\n",
    "    \n",
    "model_save_path = f'./bert_crop.bin'\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "845b8b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:18:38.970064Z",
     "iopub.status.busy": "2024-07-23T14:18:38.969642Z",
     "iopub.status.idle": "2024-07-23T14:18:38.979785Z",
     "shell.execute_reply": "2024-07-23T14:18:38.978802Z",
     "shell.execute_reply.started": "2024-07-23T14:18:38.970027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='bert_crop.bin' target='_blank'>bert_crop.bin</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/bert_crop.bin"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink('bert_crop.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79977c3a-70d9-40f4-94f6-ba50ae2d3c9e",
   "metadata": {},
   "source": [
    "## Prediction and Evaluation\n",
    "\n",
    "In the end, I make predictions on the training and test datasets and print a classification report and confusion matrix to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "832cc90e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:18:38.981630Z",
     "iopub.status.busy": "2024-07-23T14:18:38.981334Z",
     "iopub.status.idle": "2024-07-23T14:18:40.860208Z",
     "shell.execute_reply": "2024-07-23T14:18:40.859194Z",
     "shell.execute_reply.started": "2024-07-23T14:18:38.981605Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=2)\n",
    "model.load_state_dict(torch.load('bert_crop.bin', map_location=torch.device('cpu')))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34578248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:18:40.861640Z",
     "iopub.status.busy": "2024-07-23T14:18:40.861337Z",
     "iopub.status.idle": "2024-07-23T14:18:40.868761Z",
     "shell.execute_reply": "2024-07-23T14:18:40.867863Z",
     "shell.execute_reply.started": "2024-07-23T14:18:40.861615Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def pred(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    \n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "    return predictions, true_labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "244765e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:18:40.870313Z",
     "iopub.status.busy": "2024-07-23T14:18:40.870037Z",
     "iopub.status.idle": "2024-07-23T14:19:12.754743Z",
     "shell.execute_reply": "2024-07-23T14:19:12.753633Z",
     "shell.execute_reply.started": "2024-07-23T14:18:40.870291Z"
    }
   },
   "outputs": [],
   "source": [
    "yhat_train, y_train = pred(model, train_loader, device)\n",
    "yhat_test, y_test = pred(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a65aab40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:19:12.756467Z",
     "iopub.status.busy": "2024-07-23T14:19:12.756144Z",
     "iopub.status.idle": "2024-07-23T14:19:12.774064Z",
     "shell.execute_reply": "2024-07-23T14:19:12.773117Z",
     "shell.execute_reply.started": "2024-07-23T14:19:12.756442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       411\n",
      "           1       0.98      0.95      0.96       429\n",
      "\n",
      "    accuracy                           0.96       840\n",
      "   macro avg       0.96      0.96      0.96       840\n",
      "weighted avg       0.96      0.96      0.96       840\n",
      "\n",
      "[[402   9]\n",
      " [ 21 408]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, yhat_train))\n",
    "print(confusion_matrix(y_train, yhat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6985d8b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T14:19:12.775400Z",
     "iopub.status.busy": "2024-07-23T14:19:12.775102Z",
     "iopub.status.idle": "2024-07-23T14:19:12.790805Z",
     "shell.execute_reply": "2024-07-23T14:19:12.789960Z",
     "shell.execute_reply.started": "2024-07-23T14:19:12.775375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       107\n",
      "           1       0.93      0.88      0.91       103\n",
      "\n",
      "    accuracy                           0.91       210\n",
      "   macro avg       0.91      0.91      0.91       210\n",
      "weighted avg       0.91      0.91      0.91       210\n",
      "\n",
      "[[100   7]\n",
      " [ 12  91]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, yhat_test))\n",
    "print(confusion_matrix(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68d92b-b0cc-4ab5-bda6-2429024934e0",
   "metadata": {},
   "source": [
    "The model demonstrates strong performance with high precision, recall, and F1-scores for both training and test datasets. \n",
    "\n",
    "The performance metrics on the test set are slightly lower than on the training set, which is expected for small dataset. F1 score for the class 1 is 0.91 on test and 0.96 on train. However, the differences in performance are not significant, suggesting that the model can generalize well."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5300523,
     "sourceId": 8811976,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
